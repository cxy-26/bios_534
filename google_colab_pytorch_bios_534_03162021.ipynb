{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google_colab_pytorch_bios_534_03162021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ICsjsRFYbw"
      },
      "source": [
        "Let's continue with the PyTorch regression to illustrate some important concepts such as how scalable PyTorch methods are particularly when applied to large datasets with many rows and/or lots of feature columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU9RrALQbEfF"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjPwQMf5bTm5"
      },
      "source": [
        "## Creating a GPU environment\n",
        "\n",
        "We can create a GPU environment on Google CoLab without too much trouble. Use the following logic to do this although if your laptop is equipped with a GPU this should also work for you. In Google Co Lab you would need to go to the **Runtime -> Change runtime type** to get it to use a GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKQApu6T8VXd",
        "outputId": "00a50ce9-8efc-4c5c-99c2-488aecf8aee3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNz4mXUEb8cr"
      },
      "source": [
        "### Creating a Regression Data Set\n",
        "\n",
        "So you can create a regression data set quite easily using scikit-learn. This is pretty straightforward. This could take a while to do depending on where you run this code. The idea here is to create a larger dataset with lots of features which mimics some at-scale data sets you might encounter in a real life situation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4CxoUe78dYv",
        "outputId": "ac2c7080-e0a1-425e-e8c3-a2e948087094"
      },
      "source": [
        "# n_features the numner of columns\n",
        "n_features = 5000\n",
        "\n",
        "# check the scikit-learn doc on make_regression\n",
        "%time X, y = make_regression(n_samples=60000,n_features=n_features)\n",
        "print(\"The shape of X is:\",X.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17.6 s, sys: 1.18 s, total: 18.8 s\n",
            "Wall time: 18.4 s\n",
            "The shape of X is: (60000, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho9UVyPyGTym"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0cDkuQUdRgU"
      },
      "source": [
        "So now we could use the default form of Regression which is part of scikit-learn which uses the OLS method. Note, that this might take up to 6 minutes at least on the typical Google CoLab environment. In fact, it might not complet at all due to memory constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFwh2I8f8f-I",
        "outputId": "e6785761-3324-4e3f-952c-7026a5e40d56"
      },
      "source": [
        "# This could take a while\n",
        "from sklearn.linear_model import LinearRegression\n",
        "%time reg = LinearRegression().fit(X, y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 13s, sys: 6.31 s, total: 5min 20s\n",
            "Wall time: 2min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ZDSbS9eDWu"
      },
      "source": [
        "So now we could implement a way to measure error in the regression model. We'll use the standard MSE (Mean Squared Error) which can easily be computed using some simple numpy functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYvxQhjLAoCD",
        "outputId": "c704c3a4-1468-458c-cdb8-80bb1ed906c9"
      },
      "source": [
        "preds = reg.predict(X)\n",
        "\n",
        "def my_mse(actual,pred):\n",
        "  import numpy as np\n",
        "  mse = np.mean((y-preds)**2)\n",
        "  return(mse)\n",
        "\n",
        "# The regression is pretty good\n",
        "print(my_mse(y,preds))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.666648443871006e-25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZzjLx-IeoV3"
      },
      "source": [
        "Next up, let's turn the data into numpy arrays and then into tensors. You've seen this before so it shouldn't be a big surprise. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jj0n5sT8kQL"
      },
      "source": [
        "import numpy as np\n",
        "X_numpy = np.array(X, dtype='float')\n",
        "y_numpy = np.array(y, dtype='float')\n",
        "\n",
        "# Make sure you have torch imported else this will fail\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "# Let's capture some characteristis of the original data\n",
        "y = y.view(y.shape[0],1)\n",
        "n_samples, n_features = X_numpy.shape\n",
        "input_size = n_features\n",
        "\n",
        "#y_numpy = y_numpy.view(y_numpy.shape[0],1)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zgx9lc1e2bY"
      },
      "source": [
        "Now we can use the function <b>TensorDataset</b> to being the tensors into torch. `TensorDataset` provides a way to create a dataset out of the data that is already loaded into memory. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7GcE_V08_0c"
      },
      "source": [
        "# Define a tensor dataset\n",
        "train_ds = TensorDataset(X, y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMl0aQtY9GXz",
        "outputId": "15ab8660-c8fd-4c50-a5f5-96e8ecd5744a"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7fb9c370f510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNOw4zJng4qK"
      },
      "source": [
        "Now we can initialize some weights and bias values that will seed our model. Note that the weights and biases will be recomputed as we move forward and backwards within the network. Torch has tools to help us do this without a lot of problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lErJiNQM_FIE",
        "outputId": "7c66b5c7-8ca4-4e1f-d591-a0dd51b772b9"
      },
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(1, n_features, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1269, -0.0605, -0.5112,  ..., -0.9342, -0.2371,  0.0181]],\n",
            "       requires_grad=True)\n",
            "tensor([-0.7067], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyhu0MRzhT2E"
      },
      "source": [
        "So here is where we can implement the matrix (tensor) operations necessary to solve the problem. Note that the first time we do this, we are likely to experience a high error rate which is to say a high MSE value. That's okay because we will look at partial derivatives with respect to the loss function to recompute weights and biases with a goal of minimizing the MSE. This is essentially using numerical methods to find the point where the gradient produces a minimum MSE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuAS8ODl_O4Q"
      },
      "source": [
        "def model(x):\n",
        "    return x @ w.t() + b\n",
        "\n",
        "import torch.nn as nn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYpS0Yw1_STI",
        "outputId": "8c0fac09-9fc6-491f-ae31-aa73d8038652"
      },
      "source": [
        "# Generate a first set of predictions\n",
        "preds = model(X)\n",
        "print(preds)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 47.3999],\n",
            "        [ 12.1222],\n",
            "        [  7.7041],\n",
            "        ...,\n",
            "        [ 14.2788],\n",
            "        [-24.1895],\n",
            "        [-37.9776]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ntv9x4J_Vg5",
        "outputId": "e5b5784b-1293-45c6-bac5-ccbbbc88cbaf"
      },
      "source": [
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return (torch.sum(diff * diff) / diff.numel())\n",
        "\n",
        "# Compute loss\n",
        "loss = mse(preds, y)\n",
        "print(loss)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(53256.7812, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFQllSu4_auT",
        "outputId": "6d1dd2a4-c5bb-4096-97e4-32cb57b10d38"
      },
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(100):\n",
        "    preds = model(X)\n",
        "    loss = mse(preds, y)\n",
        "    print(loss,\"\\n\")\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "#        w -= w.grad * 1e-5\n",
        "        w -= w.grad * 0.25\n",
        "#        b -= b.grad * 1e-5\n",
        "        b -= b.grad * 0.25\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(53256.7812, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(12124.2549, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3706.6060, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1336.5488, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(530.9936, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(224.4765, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(99.0591, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(45.1084, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(21.0396, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(10.0010, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(4.8273, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.3598, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.1660, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.5813, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.2921, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.1478, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0752, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0385, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0198, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0102, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0053, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0028, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0014, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0008, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0004, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0002, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(0.0001, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(5.8003e-05, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0709e-05, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.6301e-05, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(8.6622e-06, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(4.6141e-06, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.4609e-06, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.3153e-06, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(7.0521e-07, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.7926e-07, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.0443e-07, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.1106e-07, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(6.0581e-08, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.3523e-08, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.8456e-08, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.0254e-08, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(6.0366e-09, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.6876e-09, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.2726e-09, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(1.3785e-09, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(9.5360e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(7.0873e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(5.5982e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(4.6715e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(4.1397e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.7832e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.5271e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.3693e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.2619e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.1841e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.1155e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0691e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0466e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0300e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0138e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0081e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9872e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9880e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9849e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9761e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9807e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9671e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9891e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9770e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9843e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9907e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9870e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9876e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9860e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9849e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9856e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9852e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9895e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0006e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9852e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9764e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9790e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9764e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9964e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9876e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0145e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0057e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0047e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9834e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0082e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0019e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0056e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0180e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9906e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0002e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9924e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0009e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(2.9915e-10, grad_fn=<DivBackward0>) \n",
            "\n",
            "tensor(3.0005e-10, grad_fn=<DivBackward0>) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DxDhVi9_gl4",
        "outputId": "84642943-a609-4ca1-c618-85833ce16053"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Define dataset\n",
        "train_ds = TensorDataset(X, y)\n",
        "train_ds[0:3]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0288,  0.3790, -0.7543,  ...,  0.5759,  0.9869, -0.7461],\n",
              "         [-2.0790, -0.5753,  1.4763,  ..., -1.3802, -0.2418,  1.5581],\n",
              "         [-1.4013,  1.7600,  0.5335,  ...,  0.6989, -0.5572, -0.0935]]),\n",
              " tensor([[ 235.2787],\n",
              "         [-352.9172],\n",
              "         [ -39.0413]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IowQczst_rnv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Define data loader\n",
        "batch_size = 1000\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb33AqIz_15l",
        "outputId": "84278606-d0cf-4383-ec18-b83487606057"
      },
      "source": [
        "# Define model\n",
        "\n",
        "model = nn.Linear(n_features, 1)\n",
        "print(model.weight)\n",
        "print(model.bias)\n",
        "\n",
        "# Parameters\n",
        "list(model.parameters())\n",
        "\n",
        "# Import nn.functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "loss = loss_fn(model(X), y, reduction=\"none\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0133,  0.0123, -0.0079,  ...,  0.0078,  0.0055,  0.0043]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0065], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr4VFhzGAD9I",
        "outputId": "7509e51b-ef5d-446b-e22a-2c99d5c45915"
      },
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.02)\n",
        "\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    \n",
        "    # Repeat for given number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # Train with batches of data\n",
        "        for xb,yb in train_dl:\n",
        "            \n",
        "            # 1. Generate predictions\n",
        "            pred = model(xb)\n",
        "            \n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(pred, yb)\n",
        "            \n",
        "            # 3. Compute gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # 4. Update parameters using gradients\n",
        "            opt.step()\n",
        "            \n",
        "            # 5. Reset the gradients to zero\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        # Print the progress\n",
        "        if (epoch+1) % 10 == 0 and loss >= 1e-12:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "            break\n",
        "        else:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "%time fit(100, model, loss_fn, opt, train_dl)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 662.2699\n",
            "Epoch [2/100], Loss: 16.5256\n",
            "Epoch [3/100], Loss: 0.6199\n",
            "Epoch [4/100], Loss: 0.0288\n",
            "Epoch [5/100], Loss: 0.0016\n",
            "Epoch [6/100], Loss: 0.0001\n",
            "Epoch [7/100], Loss: 0.0000\n",
            "Epoch [8/100], Loss: 0.0000\n",
            "Epoch [9/100], Loss: 0.0000\n",
            "Epoch [10/100], Loss: 0.0000\n",
            "CPU times: user 7.87 s, sys: 97.6 ms, total: 7.97 s\n",
            "Wall time: 7.98 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2k3TF24AF5C"
      },
      "source": [
        "# Note that you could use the OOP approach to define the model if desired\n",
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    prediction = self.linear(x)\n",
        "    return prediction"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-q2qVHCPmo"
      },
      "source": [
        "# Define model\n",
        "model = LinearRegression(n_features, 1)\n",
        "\n",
        "# Parameters\n",
        "list(model.parameters())\n",
        "\n",
        "# Import nn.functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "loss = loss_fn(model(X), y, reduction=\"none\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6LRf08seUe6",
        "outputId": "4a1100b4-f9e2-470b-c1d8-34a62a53c192"
      },
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.02)\n",
        "\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    \n",
        "    # Repeat for given number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # Train with batches of data\n",
        "        for xb,yb in train_dl:\n",
        "            \n",
        "            # 1. Generate predictions\n",
        "            pred = model(xb)\n",
        "            \n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(pred, yb)\n",
        "            \n",
        "            # 3. Compute gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # 4. Update parameters using gradients\n",
        "            opt.step()\n",
        "            \n",
        "            # 5. Reset the gradients to zero\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        # Print the progress\n",
        "        if (epoch+1) % 10 == 0 and loss >= 1e-12:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "            break\n",
        "        else:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "%time fit(100, model, loss_fn, opt, train_dl)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 613.2085\n",
            "Epoch [2/100], Loss: 17.2481\n",
            "Epoch [3/100], Loss: 0.6461\n",
            "Epoch [4/100], Loss: 0.0315\n",
            "Epoch [5/100], Loss: 0.0016\n",
            "Epoch [6/100], Loss: 0.0001\n",
            "Epoch [7/100], Loss: 0.0000\n",
            "Epoch [8/100], Loss: 0.0000\n",
            "Epoch [9/100], Loss: 0.0000\n",
            "Epoch [10/100], Loss: 0.0000\n",
            "CPU times: user 8.36 s, sys: 101 ms, total: 8.46 s\n",
            "Wall time: 8.45 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Package\n",
    "\n",
    "Steve Pittard wsp@emory.edu (citations to other sources are inline) \n",
    "\n",
    "By now you are probably fatigued with understanding the details of writing the code to split data, doing Cross Validation, storing the results, and looking at descriptive stats associated with the resulting RMSE. And this is all before considering the various parameters associated with whatever method we wish to implement.\n",
    "\n",
    "Each function has its own set of requirements which may not extend to other functions. What we need (well, what we would like) is a framework to streamline this process and automate it as much as possible but not at the expense of understanding the results.\n",
    "\n",
    "There is a module called [**scikit-learn**](https://scikit-learn.org/stable/) which can help make Machine learning a great deal easier by automating many activities that would otherwise be tedious and error prone. \n",
    "\n",
    "<img src=\"pics/scikit.png\" width =\"600\" height=600>\n",
    "\n",
    "From the project Github:\n",
    "\n",
    "> scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us page for a list of core contributors.\n",
    "\n",
    "The scikit-lean learn module provides a uniform interface for calling different algorithms while simplifying the data splitting and calculation of various performance measures. It supports many different model types and also provides the ability to tune hyper parameters. Here are some of the features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification:  SVM, nearest neighbors, random forest\n",
    "- Regression:  SVR, nearest neighbors, random forest\n",
    "- Clustering:  k-Means, spectral clustering, mean-shift\n",
    "- Preprocessing:  Transformation, scaling, feature extraction\n",
    "- Dimensionality Reduction:  k-Means, feature selection, non-negative matrix factorization\n",
    "- Model Selection:  Grid search, cross validation, metrics\n",
    "\n",
    "## Compared to R\n",
    "\n",
    "scikit-learn is not really a direct competitor to anything in R, at least in my opinion. R is a statistical framrwork first and formost which means that it has always had many statistical functions, and what we now consider to be Machine Learning methods, readily available. Over time, R has offered packages like [caret](http://topepo.github.io/caret/index.html), [tidymodels](https://www.tidymodels.org/), and [mlr](https://mlr.mlr-org.com/) (now deprecated), which represent either front ends to standalone ML packages or a rewrite of various methods. In this regard, R has had a big head start on Python. Keep in mind that data frames, which are a native object in R, are not native to Python and it was only when the Pandas module was created, that we could get similar capability. \n",
    "\n",
    "Moreover, there is a wealth of online support for these various packages which simplifies ML projects in R. There is an excellent book called [Applied Predictive](http://appliedpredictivemodeling.com/) Modeling which, though it uses the caret pacakge as a default framework, provides a great overview of general considerations relative to Machine Learning and Predictive Modeling. \n",
    "\n",
    "All this said, the scikit-learn module leverages the fact that Python was and is an easy-to-use general programming language. This makes Python a great language to develop and deploy models. The scikit-learn module is an attempt at a ground-up framework to provide ML capability. While there have been other such Python implementations, scikit-learn has the current \"mind share\" in this space. In my view, if you are going to do any type of Predictive Modeling or Machine Learning in Python then scikit-learn is the way to go.\n",
    "\n",
    "### Graphics\n",
    "\n",
    "I won't hide my opinion here that I think that R graphics, particularly ggplot2, is superior to Python graphics such as matplotlib and seaborn. (There is a reason that the [plotnine](https://towardsdatascience.com/how-to-use-ggplot2-in-python-74ab8adec129) package exists to give ggplot2 capability to Python users). However, if you are going to be working in the Python universe then you should spend time understanding matplotlib and seaborn because much of the documentation and support literature you will encounter will make reference to these frameworks. Also if you have a MATLAB background then you already have some experience with matplotlib. The scikit-learn package has a number of built-in plot types that are peculiar to various algorithms so before you try to produce a plot you might first check to see if something already exists before trying to make your own plots. \n",
    "\n",
    "## Putting scikit-learn to Work\n",
    "\n",
    "It’s easy to get lost in all that we have been doing up until this point so let’s review what the typical predictive modeling workflow will look like. \n",
    "\n",
    "   - Data Import (read in csv files, extractfrom a database, read from internet)\n",
    "   - Do some Data Visualization\n",
    "   - Data Prep (We haven’t done much of this just yet) \n",
    "  \n",
    "       * Find Missing Data and perform imputation \n",
    "       * Scaling \n",
    "       * Create dummy variables\n",
    "       * One hot encoding\n",
    "       * Dimensionality Reduction\n",
    "   \n",
    "   \n",
    "   - Data Splitting (training / test) \n",
    "   - Determine split ratio - K-Fold Cross Validation (repeated)\n",
    "   - Modeling / Prediction\n",
    "   - Evaluation\n",
    "   \n",
    "## Back To The Beginning\n",
    "\n",
    "It is implied that in predictive modeling the ultimate goal is to generate a model that could be reasonably applied to new data. As we have learned, it is best to train any model on a data set that has been (re)sampled in some way (e.g. K Fold CV) which should help provide a more realistic estimate of “out of sample” error.\n",
    "\n",
    "In our earliest example we tried to predict the MPG from mtcars using a basic linear modeling function. The sci-kit learn library provides a way to do this which allows us to easily substitute alternative functions without having to majorly change our code.\n",
    "\n",
    "As part of creating a model we can \"score\" the result using a specified performance measure. Before we do that, however, we’ll make a test / train pair. One way to do this is write our own functions to sample some proportion of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the data frame is: 32\n",
      "Number of rows in training set: 25\n",
      "Number of rows in test set: 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv\"\n",
    "mtcars = pd.read_csv(url)\n",
    "\n",
    "# How many rows ?\n",
    "print(\"Number of rows in the data frame is:\",len(mtcars))\n",
    "\n",
    "# Let's sample 80% of the data into a training data set\n",
    "prop = int(.8*len(mtcars))\n",
    "train = mtcars[0:prop]\n",
    "print(\"Number of rows in training set:\", len(train))\n",
    "\n",
    "# Let's sample 20% of the data into a test data set\n",
    "test = mtcars[prop:prop+(len(mtcars)-prop)]\n",
    "print(\"Number of rows in test set:\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of taining is: 25\n",
      "Length of test is: 7\n"
     ]
    }
   ],
   "source": [
    "def sampler(df=mtcars,prop=.80):\n",
    "    \n",
    "    # Let's sample 80% of the data into a training data set\n",
    "    prop = int(.8*len(df))\n",
    "    train = df[0:prop]\n",
    "   \n",
    "    # Let's sample 20% of the data into a test data set\n",
    "    test = df[prop:prop+(len(df)-prop)]\n",
    "    \n",
    "    return([train,test])\n",
    "\n",
    "train, test = sampler()\n",
    "print(\"Length of taining is:\",len(train))\n",
    "print(\"Length of test is:\",len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could actually write a helper function to do things like shuffle the data frame before we sample or specify whether to use boostrap sampling - if we wanted to. This winds up taking more work but gives us some generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mtcars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0677705249c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmtcars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mtcars' is not defined"
     ]
    }
   ],
   "source": [
    "def shuffler(length=32, boot=False):\n",
    "    import random\n",
    "    randomList = []\n",
    "    done = False\n",
    "    \n",
    "    if (boot):\n",
    "        for ii in range(0,length):\n",
    "            randomList.append(random.randint(0,(length-1)))\n",
    "    else:\n",
    "        while not done:\n",
    "            mynum = random.randint(0,(length-1))\n",
    "            if (mynum not in randomList or len(randomList) < (length-1)):\n",
    "                randomList.append(mynum)\n",
    "            else:\n",
    "                done = True    \n",
    "    return(randomList)\n",
    "\n",
    "shuffledList = shuffler()\n",
    "\n",
    "\n",
    "def sampler(df=mtcars,prop=.80,shuffle=False,boot=False):\n",
    "    \n",
    "    if (shuffle,boot):\n",
    "        shuffledList = shuffler(len(df))\n",
    "        df = df.iloc[shuffledList]\n",
    "    \n",
    "    # Let's sample 80% of the data into a training data set\n",
    "    prop = int(.8*len(df))\n",
    "    train = df[0:prop]\n",
    "   \n",
    "    # Let's sample x% of the data into a test data set\n",
    "    test = df[prop:prop+(len(df)-prop)]\n",
    "    \n",
    "    return([train,test])\n",
    "\n",
    "train, test = sampler(prop=.8,boot=True)\n",
    "print(\"Length of taining is:\",len(train))\n",
    "print(\"Length of test is:\",len(test))\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Way\n",
    "\n",
    "So if you've been paying attention in the previous lectures you will remember that I had use a method called \"sample\" with respect to Pandas dataframes. So why would you write your own sampling functions when you have something better and easier at your disposal ? This is entirely true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.sample(n=len(mtcars)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsampler(df=mtcars,boot=False,prop=.8):\n",
    "    \n",
    "    if (boot):    \n",
    "        df = df.sample(n=len(df),replace=True)\n",
    "    else:\n",
    "        df = df.sample(n=len(df))\n",
    "    \n",
    "    # Let's sample prop% of the data into a training data set\n",
    "    prop = int(prop*len(df))\n",
    "    train = df[0:prop]\n",
    "   \n",
    "    # Let's sample the rest of the data into a test data set\n",
    "    test = df[prop:prop+(len(df)-prop)]\n",
    "    \n",
    "    return([train,test])\n",
    "\n",
    "train, test = newsampler(prop=.7,boot=True)\n",
    "test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could now use this to model the training data and later apply it to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "train, test = newsampler()\n",
    "\n",
    "def rmse(actual,predictions):\n",
    "   from math import sqrt\n",
    "   myrmse = sqrt(((predictions-actual)**2).mean())\n",
    "   return round(myrmse,3)\n",
    "\n",
    "# Do regression and figure out rmse for training \n",
    "    \n",
    "result = sm.OLS(train.mpg,sm.add_constant(train.wt)).fit()\n",
    "train_rmse = rmse(train.mpg,result.predict(sm.add_constant(train.wt)))\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "\n",
    "test_rmse  = rmse(test.mpg,result.predict(sm.add_constant(test.wt)))\n",
    "print(\"Test RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit to The Rescue\n",
    "\n",
    "So we wrote a function to sample some proportion of a data frame to create a training data set and to also have a test data set comprised of the remaining data. Common training and test proportions include 80 / 20, 70 / 30, and 60 / 40. We didn't really need to write a function as the scikit-learn package has a function that will us do this. We'll also introduce a convention used in Python Machine Learning where we split the label (what is being predicted) into a separate structure. \n",
    "\n",
    "The remaining information represents the predictor variable(s). In the parlance of the Python, we refer to the former as \"y\" (lower case) and the latter as \"X\" (upper case). We do this in large part because there is a helper function called **train_test_split** which expects this format to do its work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = mtcars.mpg\n",
    "X = mtcars.drop('mpg',axis=1)\n",
    "\n",
    "# Next we create a training and test pair with 80 / 20 proportions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "print(\"Dimensions of X_train:\", X_train.shape)\n",
    "print(\"Dimensions of X_test\", X_test.shape)\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "# Now we can use this data with our OLD model\n",
    "\n",
    "result = sm.OLS(y_train,sm.add_constant(X_train.wt)).fit()\n",
    "train_rmse = rmse(y_train,result.predict(sm.add_constant(X_train.wt)))\n",
    "test_rmse  = rmse(y_test,result.predict(sm.add_constant(X_test.wt)))\n",
    "\n",
    "print(\"Train and Test RMSE are:\",(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is helpful in that we don't need to write our own sampler function. In fact, if you look for examples on Google you will rarely see situations wherein someone created their own sample function although it is not a bad thing to do. It's just that the ML community prefers to leverage what scikit-learn has to offer so you will see the **train_test_split** function used extensively. \n",
    "\n",
    "Of course, running one iteration on a train / test pair isn't very useful. We already discovered that K-fold cross validation is a technique to split up a data set into K-folds and systematically build training sets out of the combined K-1 folds while use the \"holdout\" fold as a test data set at some point in the process. This helps address situations wherein large variation due to outliers might be present in one fold but not another. the ultimatel goal is to build a model on a number of training data sets with the hopes that we can better predict model performance on unseen data. Here were use a helper function called **KFold** to provide us with  4 folds. The function handles the chopping of the data for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Split the \n",
    "kf = KFold(n_splits=4)\n",
    "print(\"Number of folds:\", kf.get_n_splits(mtcars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can look at the indices generated by **KFold**. We can then train our model on each of the \"combined\" folds and then apply it to the test fold. In this case we have 4 folds but we could experiment with various numbers. Our data isn't that large so we won't do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(mtcars):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual,predictions):\n",
    "   from math import sqrt\n",
    "   myrmse = sqrt(((predictions-actual)**2).mean())\n",
    "   return round(myrmse,3)\n",
    "\n",
    "X = mtcars.drop('mpg',axis=1)\n",
    "y = mtcars.mpg\n",
    "\n",
    "scores = []\n",
    "\n",
    "for fold_number, (train_index, test_index) in enumerate(kf.split(mtcars)):\n",
    "\n",
    "    # Do regression and figure out rmse for training and test\n",
    "    \n",
    "    X_train, X_test = mtcars.iloc[train_index], mtcars.iloc[test_index]\n",
    "\n",
    "    result = sm.OLS(X_train.mpg,sm.add_constant(X_train.wt)).fit()\n",
    "    train_rmse = rmse(X_train.mpg,result.predict(sm.add_constant(X_train.wt)))\n",
    "    test_rmse  = rmse(X_test.mpg,result.predict(sm.add_constant(X_test.wt)))\n",
    "    \n",
    "    scores.append([fold_number,train_rmse,test_rmse])\n",
    "    \n",
    "rmse_errors = pd.DataFrame(scores,columns=['fold_number','train','test'])\n",
    "print(rmse_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(rmse_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_errors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Better\n",
    "\n",
    "So this nice in that the **KFolds** function does the chopping up of the data for us. We don't have to write our own function. One thing that the Machine Learning does when using Python is to separate what is being predicted, in this case the **mpg** variable, from the data being used to make the prediction. One typically uses the nomenclature of **y** (lower case) to refer to the former and **X** (upper case) to refer to the latter. Note that while this isn't strictly necessary, at least as far as I can determine, it is a popular convention. So I'll just rewrite what we have above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual,predictions):\n",
    "   from math import sqrt\n",
    "   myrmse = sqrt(((predictions-actual)**2).mean())\n",
    "   return round(myrmse,3)\n",
    "\n",
    "X = mtcars.drop('mpg',axis=1)\n",
    "y = mtcars.mpg\n",
    "\n",
    "scores = []\n",
    "import statsmodels.api as sm\n",
    "for fold_number, (train_index, test_index) in enumerate(kf.split(mtcars)):\n",
    "    \n",
    "    ## Get Training Matrix and Vector\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    ## Get Testing Matrix Values\n",
    "\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    result = sm.OLS(y_train,sm.add_constant(X_train.wt)).fit()\n",
    "    train_rmse = rmse(y_train,result.predict(sm.add_constant(X_train.wt)))\n",
    "    test_rmse  = rmse(y_test,result.predict(sm.add_constant(X_test.wt)))\n",
    "    \n",
    "    scores.append([fold_number,train_rmse,test_rmse])\n",
    "    \n",
    "rmse_errors = pd.DataFrame(scores,columns=['fold_number','train','test'])\n",
    "print(rmse_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also understand that there are performance metrics available to using the **sklearn.metrics** functions to help us. We can get the Mean Sqaured Error and then take the square root of that to obtain the RMSE / Root Mean Squared Error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = mtcars.drop('mpg',axis=1)\n",
    "y = mtcars.mpg\n",
    "\n",
    "scores = []\n",
    "import statsmodels.api as sm\n",
    "for fold_number, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    ## Get Training Matrix and Vector\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    ## Get Testing Matrix Values\n",
    "\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    \n",
    "#   X_train, X_test = mtcars.iloc[train_index], mtcars.iloc[test_index]\n",
    "\n",
    "    result    = sm.OLS(y_train,sm.add_constant(X_train.wt)).fit()\n",
    "    train_mse = mean_squared_error(y_train,result.predict(sm.add_constant(X_train.wt)))\n",
    "    test_mse  = mean_squared_error(y_test,result.predict(sm.add_constant(X_test.wt)))\n",
    "    \n",
    "    train_rmse = train_mse**0.5\n",
    "    test_rmse  = test_mse**0.5\n",
    "    \n",
    "    scores.append([fold_number,train_rmse,test_rmse])\n",
    "    \n",
    "rmse_errors = pd.DataFrame(scores,columns=['fold_number','train','test'])\n",
    "print(rmse_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "So it turns out that scikit-learn has its own methods for Linear Regression that include various options such as to use Ridge or Lasso. We'll now replace the simple OLS method with something from scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "X = mtcars.wt\n",
    "y = mtcars.mpg\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for fold_number, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    ## Get Training Matrix and Vector\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y[train_index].values.reshape(-1,1)\n",
    "\n",
    "    ## Get Testing Matrix Values\n",
    "\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index].values.reshape(-1,1)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    reg = model.fit(X_train.values.reshape(-1,1),y_train)\n",
    "    \n",
    "    y_train_preds = model.predict(X_train.values.reshape(-1,1))\n",
    "    y_test_preds = model.predict(X_test.values.reshape(-1,1))\n",
    "     \n",
    "    train_mse = mean_squared_error(y_train,y_train_preds)\n",
    "    test_mse  = mean_squared_error(y_test,y_test_preds)\n",
    "    \n",
    "    train_rmse = math.sqrt(train_mse)\n",
    "    test_rmse = math.sqrt(test_mse)\n",
    "    \n",
    "    scores.append([fold_number,train_rmse,test_rmse])\n",
    "    \n",
    "rmse_errors = pd.DataFrame(scores,columns=['fold_number','train_rmse','test_rmse'])\n",
    "print(rmse_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing with scikit-learn functions is that they want input in a particular format so some type of conversion is ususally required which is why you saw things like the **X_test.values.reshape(-1,1)** statement above. Sometimes it's easier to convert the pandas data frame into an array. In this example, we'll do just the regression piece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv\"\n",
    "mtcars = pd.read_csv(url)\n",
    "\n",
    "mtvals = mtcars.values            # creates a numpy array\n",
    "type(mtvals)                      # Observe the data type\n",
    "\n",
    "y = mtvals[:,0]         # Gets the mpg column\n",
    "X = mtvals[:,1:10]      # Gets everything BUT the mpg column\n",
    "\n",
    "# Setup a new model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "reg = model.fit(X,y)\n",
    "\n",
    "# Look at the equation coeeficients \n",
    "print(\"Regression equation coefficients:\\n\", reg.coef_)\n",
    "\n",
    "# Look at the intercept\n",
    "print(\"\\nRegression equation intercept:\",reg.intercept_)\n",
    "\n",
    "# Do a prediction\n",
    "y_preds = reg.predict(X)\n",
    "\n",
    "# Get mean sqaured error\n",
    "print('Root Mean squared error: %.2f'\n",
    "      % np.sqrt(mean_squared_error(y, y_preds)))\n",
    "\n",
    "# Get R^2\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be helpful to know that most models have a default score method that defaults to something that makes sense for the method being used. In the case of regression, the default scoring metric is the R^2 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X,y).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Small Review\n",
    "\n",
    "So now want to clean all of this up into something that can serve as an example for your future work. Things are kind of messy right now so let's review what it is we would generally want to do:\n",
    "\n",
    "1. Split a data frame into what is being predicted vs the predictor variables\n",
    "2. Use the training data as a source against which to do the cross fold valiation\n",
    "3. For each fold, apply whatever model you want (e.g. Linear Regression) and capture metrics\n",
    "4. Look at the performance metric(s) (e.g. RMSE) across each fold or in aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this is write a function that implements the above. It's not hard to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "url = \"https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv\"\n",
    "mtcars = pd.read_csv(url)\n",
    "\n",
    "rmse_train_info = []\n",
    "rmse_test_info = []\n",
    "\n",
    "mtvals = mtcars.values            # creates a numpy array\n",
    "\n",
    "y = mtvals[:,0]         # Gets the mpg column\n",
    "X = mtvals[:,1:10]      # Gets everything BUT the mpg column\n",
    "\n",
    "kfold = KFold(3)\n",
    "\n",
    "# Main processing loop for the folds\n",
    "\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "  # split data coming from each of the 4 folds\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize a model\n",
    "        regress = LinearRegression()\n",
    "        \n",
    "        # Fit the Model\n",
    "        regress.fit(X_train,Y_train)\n",
    "        \n",
    "        # Predict the target on the training dataset\n",
    "        predict_train = regress.predict(X_train)\n",
    "  \n",
    "        # Root Mean Squared Error on training dataset\n",
    "        rmse_train = mean_squared_error(Y_train,predict_train)**(0.5)\n",
    "        print(\"Training RMSE for loop\",ii,\"is:\", rmse_train)\n",
    "       \n",
    "        # Append the rmse to the rmse_train_info vector\n",
    "        rmse_train_info.append(rmse_train)\n",
    "        \n",
    "        # Now let's do a prediction on the test data\n",
    "        predict_test = regress.predict(X_test)\n",
    "        \n",
    "        # Root Mean Squared Error on training dataset\n",
    "        rmse_test = mean_squared_error(Y_test,predict_test)**(0.5)\n",
    "        print(\"Test RMSE for loop\",ii,\"is:\", rmse_test,\"\\n\")\n",
    "       \n",
    "        # Append the rmse to the rmse_test_info vector\n",
    "        rmse_test_info.append(rmse_test)\n",
    "\n",
    "        ii = ii+1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easier Cross Validation\n",
    "\n",
    "Or, we could look at what scikit-learn can do for us. The following introduces the  **cross_val_score** and **cross_validate** functions which provide a wrapper around a given **kfolds** object while also applying a predefined scoring method. While this approach results in less code it also provides less transparency. So what we do here is:\n",
    "\n",
    "  - Create a Model object (e.g. Regression)\n",
    "  - Specify one or more scoring metrics\n",
    "  - Create a KFold object with at least two folds\n",
    "  - Pass the above information to the cross_validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "mtvals = mtcars.values  # Create an array version of the data\n",
    "y = mtvals[:,0]         # Gets the mpg column\n",
    "X = mtvals[:,1:10]      # Gets everything BUT the mpg column\n",
    "\n",
    "# Setup the model \n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# We have to designate a scoring metric \n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up some folds\n",
    "kfold = KFold(n_splits=4)\n",
    "\n",
    "# The cross_validate function handles the execution of the model\n",
    "# as well as the management of the folds\n",
    "\n",
    "cv  = cross_validate(model,X,y,scoring=(mse), cv=kfold, return_train_score=True)\n",
    "\n",
    "# Here we print out the rmse of the test / holdout data\n",
    "print(\"Test RMSE per fold:\",(cv['test_score']**0.5))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Test RMSE:\",(cv['test_score']**0.5).mean().round(2))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Train RMSE:\",(cv['train_score']**0.5).mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice side effect of this approach is that we can easily substitute in other methods as long as they make sense of course. In other words, since we are predicting a continuous outcome we need to select a method that supports this intent. Let's try Ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.linear_model import Ridge\n",
    "    \n",
    "# Setup the model \n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "# We have to designate a scoring metric \n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up some folds\n",
    "kfold = KFold(n_splits=4)\n",
    "\n",
    "# The cross_validate function handles the execution of the model\n",
    "# as well as the management of the folds\n",
    "\n",
    "cv  = cross_validate(model,X,y,scoring=(mse), cv=kfold, return_train_score=True)\n",
    "\n",
    "# Here we print out the rmse of the test / holdout data\n",
    "print(\"Test RMSE per fold:\",(cv['test_score']**0.5))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Test RMSE:\",(cv['test_score']**0.5).mean().round(2))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Train RMSE:\",(cv['train_score']**0.5).mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could always use Repeated Cross Fold Validation. This is fairly straightforward but you have to know what function to call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "# We have to designate a scoring metric \n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up some folds\n",
    "repeat_kfold = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "# The cross_validate function handles the execution of the model\n",
    "# as well as the management of the folds\n",
    "\n",
    "cv  = cross_validate(model,X,y,scoring=(mse), cv=repeat_kfold, return_train_score=True)\n",
    "\n",
    "# Here we print out the rmse of the test / holdout data\n",
    "print(\"Test RMSE per fold:\",(cv['test_score']**0.5))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Test RMSE:\",(cv['test_score']**0.5).mean().round(2))\n",
    "\n",
    "# Look at the mean of this RMSE array\n",
    "print(\"Mean Train RMSE:\",(cv['train_score']**0.5).mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is important to know that there are a number of built in scoring metrics available for regression and classification. These are available to assist with the evaluation of any models that you make and could simplify your work. Check the [reference](https://scikit-learn.org/stable/modules/model_evaluation.html) page for a more detailed discussion of such metrics. \n",
    "\n",
    "<img src=\"pics/rm.png\" width =\"600\" height=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge()\n",
    "\n",
    "# These metrics are known to scikit\n",
    "scoring = {'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "           'r2':'r2'}\n",
    "\n",
    "# Set up some folds\n",
    "repeat_kfold = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "# The cross_validate function handles the execution of the model\n",
    "# as well as the management of the folds\n",
    "\n",
    "cv  = cross_validate(model,\n",
    "                     X,y,\n",
    "                     scoring=scoring, \n",
    "                     cv=repeat_kfold, \n",
    "                     return_train_score=True)\n",
    "\n",
    "# Here we print out the rmse of the test / holdout data\n",
    "print(\"Test RMSE per fold:\",np.sqrt(np.abs(cv['test_neg_mean_squared_error'])))\n",
    "\n",
    "# Here we print out the r2of the test / holdout data\n",
    "print(\"Test R2 per fold:\",cv['test_r2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamaters\n",
    "\n",
    "Linear Regression is failry straighforward and there aren't many arguments to supply to the function although there are a couple of “hyperparameters” which are arguments to a given method that you can set before you call the method. In this case there are two hyperparameters called \"alpha\" and \"solver\" that assume default variables if we don’t supply values. There is even a function called **RidgeCV** which will handle some of this stuff for us although it is specific to the **Ridge** method. Here is how we could use it to find the best value for **alpha**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "# We have to designate a scoring metric \n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# define search\n",
    "search = RidgeCV(alphas=np.arange(0, 1, 0.1), cv=cv, scoring=(mse))\n",
    "\n",
    "# perform the search\n",
    "results_ridge = search.fit(X, y)\n",
    "\n",
    "print('RidgeCV RMSE: %.3f' % results_ridge.best_score_**0.5)\n",
    "print('RidgeCV Best Alpha: %s' % results_ridge.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call a more general function to handle the search for an ideal set of parameters in relation to optimizing target metric. If our desired score is to minimize the RMSE then we can indicate this in the scoring paramter. We can also provide a parameter grid with values we want to be evaluated as part of this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create model\n",
    "model = Ridge()\n",
    "\n",
    "# Create a grid of parameters to be evaluated. Note that\n",
    "# this is specific to the method being used\n",
    "param_grid={'alpha': np.arange(0,1,0.1)}\n",
    "\n",
    "# do the grid search\n",
    "search = GridSearchCV(model, param_grid, scoring=(mse), cv=cv, n_jobs=-1)\n",
    "\n",
    "# Get the results\n",
    "results_gridsearch = search.fit(X, y)\n",
    "\n",
    "# summarize\n",
    "print('Grid_Search RMSE: %.3f' % results_gridsearch.best_score_**0.5)\n",
    "print('Grid_Search Config: %s' % results_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's expand the parameter dictionary to include \"solver\"\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "# Here we add in additional parameters into the grid\n",
    "param_grid={'alpha': np.arange(0,1,0.1),\n",
    "            'solver': ['svd', 'cholesky', 'lsqr']}\n",
    "\n",
    "search = GridSearchCV(model, param_grid, scoring=(mse), cv=cv, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results_gridsearch = search.fit(X, y)\n",
    "\n",
    "# summarize\n",
    "print('Grid_Search RMSE: %.3f' % results_gridsearch.best_score_**0.5)\n",
    "print('Grid_Search Best Parameter Config: %s' % results_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Input Formats\n",
    "\n",
    "One of the more challening things when working with data is that some methods seem to want the input in a certain format. Let's revisit the opening section when we separated the mtcars data into X and y which is the preferred way of indicating what is being used to predict and outcome. In using various scikit functions, I find myself having to reshape the data or turning it into an array to accomodate the input requirements of various functions. In all fairness, I have had to do the same thing when using R so it's not unique to a given language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mtcars.mpg\n",
    "X = mtcars.drop('mpg',axis=1)\n",
    "\n",
    "# X is a pandas dataframe\n",
    "print(\"X is of type:\",type(X))\n",
    "\n",
    "# y is a Series which is comparable to a vector in R\n",
    "print(\"y is of type:\",type(y))\n",
    "\n",
    "# Next we create a training and test pair with 80 / 20 proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create and fit the model\n",
    "lr_mod = LinearRegression()\n",
    "lr_mod.fit(X_train,y_train)\n",
    "\n",
    "# Do some scoring\n",
    "y_lr_preds = lr_mod.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,y_lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So compare this to the version we used ealrier when we first converted the mtcars data frame into a numpy array. At this point things still work although we have lost the feature names in the X input which is now an array. You might feel that this is academic but later when we look at classification predictions, you will see that knowing how to reshape and manipulate X and y will come in handy as you use various functions in scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtvals = mtcars.values            # creates a numpy array\n",
    "\n",
    "# note we have to index by number since the names are no longer available\n",
    "y = mtvals[:,0]         # Gets the mpg column\n",
    "X = mtvals[:,1:10]      # Gets everything BUT the mpg column\n",
    "\n",
    "# X is a numpy array\n",
    "print(\"X is of type:\",type(X))\n",
    "\n",
    "# y is a numpy array\n",
    "print(\"y is of type:\",type(y))\n",
    "\n",
    "# Next we create a training and test pair with 80 / 20 proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create and fit the model\n",
    "lr_mod = LinearRegression()\n",
    "lr_mod.fit(X_train,y_train)\n",
    "\n",
    "# Do some scoring\n",
    "y_lr_preds = lr_mod.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,y_lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mod.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Null Model\n",
    "\n",
    "Keep in mind that we are assuming that whatever we are doing (or plan to do) is better than doing little or nothing. That is, we could just make our model reflect the average MPG of the training data and leave it at that. After all, why go to the trouble of writing code and testing out different methods if we could do just as well by going with the average MPG approach. scikit-learn has what is known as a \"dummy\" model function that can implement this \"method\". First, check out this graph which shows us how the Null model might look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(mtcars.wt,mtcars.mpg)\n",
    "plt.axhline(np.mean(mtcars.mpg), color=\"red\")\n",
    "plt.grid()\n",
    "plt.title(\"Null Model - Mean MPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement the model using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import what we need - could be redundant but\n",
    "# this will show you what you need to reproduce this\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "from numpy import sqrt\n",
    "\n",
    "# Reread the mtcars file\n",
    "url = \"https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv\"\n",
    "mtcars = pd.read_csv(url)\n",
    "\n",
    "# Create the X, y combo\n",
    "X = mtcars.drop(\"mpg\",axis=1)\n",
    "y = mtcars.mpg\n",
    "\n",
    "# Now we make a Dummy model that uses the mean of the dependent variable\n",
    "dummy_mean = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Set up a scores list to collect RMSE as we do KFold validation\n",
    "scores = []\n",
    "\n",
    "# try with KFold\n",
    "kf = KFold(n_splits=4,shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "     dummy_mean.fit(X_train,y_train)\n",
    "     ypreds = dummy_mean.predict(X_test)\n",
    "     scores.append(mean_squared_error(y_test,ypreds,squared=False))\n",
    "\n",
    "print(\"Mean RMSE is %f\" % mean(scores))\n",
    "\n",
    "# The RMSE emerging from the following  should be comparable to what we did above \n",
    "# After all, this is just an automatic way of doing the above that uses the same\n",
    "# folds created by the KFold object\n",
    "\n",
    "cross_val_scores = cross_val_score(dummy_mean, \n",
    "                                   X, \n",
    "                                   y, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   cv=kf, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "print(\"Mean RMSE for cross_val_score is %f \" % sqrt(mean(absolute(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the RMSE is not particularly different. After all, we used the same folds and the second approach just implemented **cross_val_score** function to make things easier albeit less transparent. The larger point here is that I wanted to show how to implement a null / baseline model using scikit. We've already looked at a number of examples involving regression and variants thereof but here is one more just to contrast this with the dummy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an actual Regresssion Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "cross_val_scores = cross_val_score(reg, \n",
    "                                   X, \n",
    "                                   y, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   cv=kf, \n",
    "                                   n_jobs=-1)\n",
    "print(\"Mean RMSE for cross_val_score is %f \" % sqrt(mean(absolute(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to know that we don't have to create a separate KFold object to do cross fold validation. We can just tell **cross_validate** or **cross_val_score** to use its own validation just by supplying the desired number of folds. Here we specify 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(reg, \n",
    "                                   X, \n",
    "                                   y, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   cv=3, \n",
    "                                   n_jobs=-1)\n",
    "print(\"Mean RMSE for cross_val_score is %f \" % sqrt(mean(absolute(cross_val_scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
